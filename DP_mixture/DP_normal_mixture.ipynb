{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as sp\n",
    "from autograd.scipy import special\n",
    "from autograd import grad, hessian, hessian_vector_product, hessian, jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DP parameters\n",
    "x_dim = 2\n",
    "k_approx = 5\n",
    "num_obs = 12\n",
    "\n",
    "# prior parameters\n",
    "alpha = 1.0 # DP parameter\n",
    "mu_prior = np.zeros(x_dim)\n",
    "mu_prior_info = 1.0 * np.eye(x_dim)\n",
    "\n",
    "info_x = 1.0 * np.eye(x_dim)\n",
    "\n",
    "# true means\n",
    "mu_spacing = np.linspace(-10, 10, k_approx)\n",
    "true_mu = np.array([ mu_spacing, mu_spacing]).T\n",
    "\n",
    "# draw beta sticks\n",
    "true_v = np.zeros(k_approx)\n",
    "true_pi = np.zeros(k_approx)\n",
    "stick_remain = np.zeros(k_approx)\n",
    "\n",
    "true_v[0] = np.random.beta(1, alpha)\n",
    "true_pi[0] = true_v[0]\n",
    "stick_remain[0] = 1 - true_v[0]\n",
    "\n",
    "for i in range(1, k_approx): \n",
    "    if i == k_approx - 1: # the last stick\n",
    "        true_v[i] = 1.0\n",
    "    else: \n",
    "        true_v[i] = np.random.beta(1, alpha)\n",
    "    \n",
    "    true_pi[i] = stick_remain[i - 1] * true_v[i]\n",
    "    stick_remain[i] = stick_remain[i - 1] * (1 - true_v[i])\n",
    "    \n",
    "\n",
    "# draw group indicators\n",
    "\"\"\"true_z = np.random.multinomial(1, true_pi, num_obs)\n",
    "true_z_ind = np.full(num_obs, -1)\n",
    "for row in np.argwhere(true_z):\n",
    "    true_z_ind[row[0]] = row[1]\n",
    "\"\"\"    \n",
    "\n",
    "true_z_ind = np.random.choice(range(k_approx), p = true_pi, size = num_obs)\n",
    "true_z = np.zeros((num_obs, k_approx))\n",
    "for i in range(num_obs): \n",
    "    true_z[i, true_z_ind[i]] = 1.0\n",
    "\n",
    "# draw observations\n",
    "x = np.array([ np.random.multivariate_normal(\n",
    "                true_mu[true_z_ind[n]], np.linalg.inv(info_x)) \\\n",
    "               for n in range(num_obs) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed mixture proportions: \n",
      " [ 0.58333333  0.41666667  0.          0.          0.        ]\n",
      "true mixture proportions: \n",
      " [ 0.49142064  0.42663463  0.07482192  0.0013834   0.00573942]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADrVJREFUeJzt3V+IW+edxvHn0aSaZUovUnvqmiTjScF74bIlUGEY6IWC\nQ+P2Yt0UWhKGJbQLsxdtL/YikDLQmobBS2HpxdIuqJA2F9oGQ8nWNCUhHhBmQUuqgWxqJ3Vq2szE\nJo0d92ZhWKuZ+e3FkZMZW/b8kY6ORu/3A+LovEfW++NEefz6PUevHBECAIy+UtEFAAAGg8AHgEQQ\n+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASARBD4AJOKeogvYaP/+/TE9PV10GQCwpywtLb0fEZNb\nvW6oAn96elqtVqvoMgBgT7G9vJ3XMaUDAIkg8AEgEQQ+ACSCwAeARBD4AJCIvgS+7WdtX7V9fkPb\nJ22/YvsPne29/egLAEZKvS5NT0ulUrat13Prql8j/J9LOn5L29OSFiPisKTFzj4A4KZ6XZqbk5aX\npYhsOzeXW+j3JfAj4pykv9zSfELSc53nz0n6Sj/6AoCRMT8vra5ubltdzdpzkOcc/oGIeLfz/M+S\nDnR7ke052y3brWvXruVYDgAMmZWVnbX3aCAXbSP7pfSuv5YeEbWIqEREZXJyy28GA8DomJraWXuP\n8gz892wflKTO9mqOfQHA3rOwIE1MbG6bmMjac5Bn4J+R9GTn+ZOSfpVjXwCw98zOSrWadOiQZGfb\nWi1rz4Gz2ZYe38T+haSqpP2S3pP0fUn/Kem0pClJy5K+HhG3XtjdpFKpBIunAcDO2F6KiMpWr+vL\napkR8cQdDh3rx/sDAHrHN20BIBEEPgAkgsAHgEQQ+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASAR\nBD4AJILAB4BEEPgAkAgCHwASQeADQCIIfABIBIEPAIkg8AEgEQQ+ACSCwAeARBD4AJAIAh8AEkHg\nA0AiCHwASASBDwCJIPABIBEEPgAkgsAHgEQQ+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASARBD4A\nJILAB4BEEPgAkIh78u7A9tuS/lfSmqQPIqKSd58AgNsNaoT/cEQ8RNgDCavXpelpqVTKtvV60RUl\nJ/cRPgCoXpfm5qTV1Wx/eTnbl6TZ2eLqSswgRvgh6aztJdtzA+gPwLCZn/8o7G9aXc3aMTCDGOF/\nISKu2P6UpFds/z4izt082PlLYE6SpqamBlAOgIFbWdlZO3KR+wg/Iq50tlclvSDp6C3HaxFRiYjK\n5ORk3uUAKMKdBnMM8gYq18C3/XHbn7j5XNIXJZ3Ps08AQ2hhQZqY2Nw2MZG1Y2DyHuEfkPRftv9H\n0quSXoyIl3LuE8CwmZ2VajXp0CHJzra1GhdsB8wRUXQNH6pUKtFqtYouAwD2FNtL27ntnW/aAkAi\nCHwASASBDwCJIPABIBEEPgAkgsAHgEQQ+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASARBD4AJILA\nB4BEEPgAkAgCHwASQeADQCIIfABIBIEPAIkg8AEgEQQ+ACSCwAeARBD4AJAIAh8AEkHgA0AiCHwA\nSASBDwCJIPABIBEEPgAkgsAHgEQQ+ACQCAIfABJB4ANAIgh8AEgEgQ8AiSDwASARBD4AJCL3wLd9\n3PZF25dsP513fwCA7nINfNtjkn4s6UuSjkh6wvaRPPsEhkK9Lk1PS6VStq3Xi64I0D05v/9RSZci\n4o+SZPt5SSckvZFzv0Bx6nVpbk5aXc32l5ezfUmanS2uLiQv7ymd+yS9s2H/cqcNGF3z8x+F/U2r\nq1k7UKDCL9ranrPdst26du1a0eUAvVtZ2Vk7MCB5B/4VSQ9s2L+/0/ahiKhFRCUiKpOTkzmXAwzA\n1NTO2oEByTvwfyvpsO0HbZclPS7pTM59AsVaWJAmJja3TUxk7UCBcg38iPhA0rclvSzpTUmnI+JC\nnn0ChZudlWo16dAhyc62tRoXbFE4R0TRNXyoUqlEq9UqugwA2FNsL0VEZavXFX7RFgAwGAQ+ACSC\nwAeARBD4AJAIAh8AEkHgA0AiCHwASASBDwCJIPABIBEEPgAkgsAHgEQQ+ACQCAIfABJB4ANAIgh8\nAEgEgQ8AiSDwASARBD6GTrPZ1KlTp9RsNosuBRgp9xRdALBRs9nUsWPH1G63VS6Xtbi4qJmZmaLL\nAkYCI3wMlUajoXa7rbW1NbXbbTUajaJLAkYGgY+hUq1WVS6XNTY2pnK5rGq1WnRJwMhgSgdDZWZm\nRouLi2o0GqpWq0znAH1E4GPozMzMEPRADpjSAYBEEPgAkAgCHwASQeADQCIIfABIBIEPAIkg8AEg\nEQQ+ACSCwEffsdolMJz4pi36itUugeHFCB99xWqXwPAi8NFXrHYJDC+mdNBXrHYJDC8CH33HapfA\ncMptSsf2SdtXbL/WeXw5r74AAFvLew7/RxHxUOfxm5z7QhHqdWl6WiqVsm29XnRFAO6AKR3sXr0u\nzc1Jq6vZ/vJyti9Js7PF1QWgq7xH+N+x/brtZ23fm3NfGLT5+Y/C/qbV1awdwNBxROz+D9tnJX26\ny6F5Sf8t6X1JIekZSQcj4ptd3mNO0pwkTU1NfX55eXnX9WDASiWp2+fHltbXB18PkCjbSxFR2ep1\nPU3pRMQj2yzmp5J+fYf3qEmqSVKlUtn93z4YvKmpbBqnWzuAoZPnXToHN+w+Jul8Xn2hIAsL0sTE\n5raJiawdwNDJcw7/h7Z/Z/t1SQ9L+ucc+0IRZmf11lNPacXWuqQVW2899RQXbIEhlVvgR8Q/RMTf\nRcTnIuLvI+LdvPpCcX45Pq7PlEoak/SZUkm/HB8vuiQAd8BaOujJndbOYYlkYPhwHz560m3tHJZI\nBoYTgY+e3bp2Trclkgl8oHhM6aDvWCIZGE6M8NF3LJEMDCcCH7lgiWRg+DClAwCJIPD3GG53BLBb\nTOnsIdzuCKAXjPD3kG63OwLAdhH4ewi3OwLoBVM6ewi3OwLoBYG/x3C7I4DdYkoHABJB4ANAIgh8\nAEgEgQ8AiSDwASARBD4AJILAB4BEEPgAkAgCHwASQeADQCIIfABIBIEPAIkg8AEgEQQ+ACSCwAeA\nRBD4AJAIAr/Pms2mTp06pWazWXQpALAJv3jVR81mU8eOHVO73Va5XNbi4iK/TgVgaDDC34U7jeIb\njYZu3LihtbU13bhxQ41Go5gCAaALRvg7dLdR/L59+7S+vi5JWl9f1759+4osFQA2YYS/Q41GQ+12\nW2tra2q325tG8devX1eplJ3SUqmk69evF1QlANyOwN+harWqcrmssbExlctlVavVTcfGx8c1Njam\n8fHxTccAoGhM6ezQzMyMFhcX1Wg0VK1WN12UvdsxACiaI6LoGj5UqVSi1WoVXQYA7Cm2lyKistXr\neprSsf012xdsr9uu3HLsu7Yv2b5o+9Fe+gEA9K7XOfzzkr4q6dzGRttHJD0u6bOSjkv6ie2xHvsq\nVr0uTU9LpVK2rdeLrggAdqSnwI+INyPiYpdDJyQ9HxE3IuJPki5JOtpLX0V66+RJtb/xDWl5WYrI\ntnNzhD6APSWvu3Tuk/TOhv3LnbY9p9ls6m9+8AOV//rXzQdWV6X5+WKKAoBd2DLwbZ+1fb7L40Q/\nCrA9Z7tlu3Xt2rV+vGVfNRoN3X+nC9srK4MtBgB6sOVtmRHxyC7e94qkBzbs399p6/b+NUk1KbtL\nZxd95apareqyraluoT81NfiCAGCX8prSOSPpcdvjth+UdFjSqzn1lauZmRn93/e+p/bHPrb5wMSE\ntLBQTFEAsAu93pb5mO3LkmYkvWj7ZUmKiAuSTkt6Q9JLkr4VEWu9FluUvz15UuWf/Uw6dEiys22t\nJs3OFl0aAGzbyHzxqtls8g1XAEna7hevRmJpBdahB4CtjcTiaXdbwRIAkBmJwL/bCpYAgMxITOn0\na5VKrgMAGGUjEfhSFvq9hDTXAQCMupGY0ukHrgMAGHUEfgfXAQCMupGZ0tloN3Px/FoVgFE3coHf\ny1x8r9cBAGCYjdyUDnPxANDdyAU+c/EA0N3ITekwFw8A3Y1c4EvMxQNANyM3pQMA6I7AB4BEEPgA\nkAgCHwASQeADQCIIfABIxFD9pq3ta5KWi65jC/slvV90EUOI89Id5+V2nJPuejkvhyJicqsXDVXg\n7wW2W9v5seDUcF6647zcjnPS3SDOC1M6AJAIAh8AEkHg71yt6AKGFOelO87L7Tgn3eV+XpjDB4BE\nMMIHgEQQ+Ntk+2u2L9het1255dh3bV+yfdH2o0XVWDTbJ21fsf1a5/Hlomsqiu3jnc/DJdtPF13P\nsLD9tu3fdT4fraLrKYrtZ21ftX1+Q9snbb9i+w+d7b397pfA377zkr4q6dzGRttHJD0u6bOSjkv6\nie2xwZc3NH4UEQ91Hr8pupgidP77/1jSlyQdkfRE53OCzMOdz0fKt2b+XFlebPS0pMWIOCxpsbPf\nVwT+NkXEmxFxscuhE5Kej4gbEfEnSZckHR1sdRgyRyVdiog/RkRb0vPKPieAJCkizkn6yy3NJyQ9\n13n+nKSv9LtfAr9390l6Z8P+5U5bqr5j+/XOP1n7/k/SPYLPxJ2FpLO2l2zPFV3MkDkQEe92nv9Z\n0oF+dzCSv3i1W7bPSvp0l0PzEfGrQdczjO52jiT9u6RnlP1P/Yykf5X0zcFVhz3gCxFxxfanJL1i\n+/ed0S42iIiw3fdbKAn8DSLikV38sSuSHtiwf3+nbSRt9xzZ/qmkX+dczrBK6jOxExFxpbO9avsF\nZdNfBH7mPdsHI+Jd2wclXe13B0zp9O6MpMdtj9t+UNJhSa8WXFMhOh/Smx5TdqE7Rb+VdNj2g7bL\nyi7qnym4psLZ/rjtT9x8LumLSvcz0s0ZSU92nj8pqe+zCozwt8n2Y5L+TdKkpBdtvxYRj0bEBdun\nJb0h6QNJ34qItSJrLdAPbT+kbErnbUn/VGw5xYiID2x/W9LLksYkPRsRFwouaxgckPSCbSnLnv+I\niJeKLakYtn8hqSppv+3Lkr4v6V8knbb9j8pWDf563/vlm7YAkAamdAAgEQQ+ACSCwAeARBD4AJAI\nAh8AEkHgA0AiCHwASASBDwCJ+H/QRWCbMY5BcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7a9b767b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the data\n",
    "plt.plot(x[:,0], x[:,1], 'k.')\n",
    "for k in range(k_approx):\n",
    "    plt.plot(true_mu[k, 0], true_mu[k, 1], 'ro')\n",
    "    \n",
    "print('observed mixture proportions: \\n', np.mean(true_z, axis = 0))\n",
    "print('true mixture proportions: \\n', true_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up VB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../LinearResponseVariationalBayes.py')\n",
    "\n",
    "from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "from VariationalBayes.Parameters import ScalarParam, VectorParam, ArrayParam\n",
    "from VariationalBayes.MultinomialParams import SimplexParam\n",
    "from VariationalBayes.DirichletParams import DirichletParamArray\n",
    "from VariationalBayes.MatrixParameters import PosDefMatrixParam, PosDefMatrixParamVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_params = ModelParamsDict('global')\n",
    "global_params.push_param(\n",
    "    PosDefMatrixParamVector(name='info', length=k_approx, matrix_size=x_dim)) # variational variances\n",
    "global_params.push_param(\n",
    "    ArrayParam(name='mu', shape=(k_approx, x_dim))) # variational means\n",
    "global_params.push_param(\n",
    "    DirichletParamArray(name='v_sticks', shape=(k_approx - 1, 2))) # note the shape k_approx - 1 ...\n",
    "                                                                   # the last stick is always 1 in our approximation\n",
    "local_params = ModelParamsDict('local')\n",
    "local_params.push_param(\n",
    "    SimplexParam(name='e_z', shape=(num_obs, k_approx)))\n",
    "\n",
    "vb_params = ModelParamsDict('vb_params model')\n",
    "vb_params.push_param(global_params)\n",
    "vb_params.push_param(local_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior_params = ModelParamsDict('prior_params')\n",
    "prior_params.push_param(VectorParam(name='mu_prior_mean', size=x_dim, val=mu_prior))\n",
    "prior_params.push_param(PosDefMatrixParam(name='mu_prior_info', size=x_dim, val=mu_prior_info))\n",
    "prior_params.push_param(ScalarParam(name='alpha', val=alpha))\n",
    "prior_params.push_param(PosDefMatrixParam(name='info_x', size=x_dim, val=info_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## initialize VB params\n",
    "\n",
    "true_init = True\n",
    "if true_init: \n",
    "    true_tau = np.zeros(np.shape(vb_params['global']['v_sticks'].alpha.get()))\n",
    "    true_tau[:,0] = true_v[:-1] * 100\n",
    "    true_tau[:,1] = (1 - true_v[:-1]) * 100\n",
    "    \n",
    "    true_z_ = deepcopy(true_z)\n",
    "    true_z_[true_z == 1] = 1 - 10**(-8)\n",
    "    true_z_[true_z == 0] = 0 + 10**(-8)\n",
    "    \n",
    "    vb_params['global']['mu'].set(true_mu)\n",
    "    vb_params['global']['v_sticks'].alpha.set(true_tau)\n",
    "    vb_params['local']['e_z'].set(true_z_)\n",
    "\n",
    "else: \n",
    "    init_tau = np.random.uniform(.1, 1, np.shape(vb_params['global']['v_sticks'].alpha.get()))\n",
    "    init_mu = np.random.uniform(-10, 10, np.shape(vb_params['global']['mu'].get()))\n",
    "    init_z = np.random.uniform(0.01, 0.99, np.shape(vb_params['local']['e_z'].get()))\n",
    "    \n",
    "    vb_params['global']['mu'].set(true_mu)\n",
    "    vb_params['global']['v_sticks'].alpha.set(init_tau)\n",
    "    vb_params['local']['e_z'].set(init_z)\n",
    "\n",
    "init_par_vec = vb_params.get_free()\n",
    "global_init_par_vec = vb_params['global'].get_free()\n",
    "    \n",
    "## initialize prior params\n",
    "prior_params['mu_prior_mean'].set(np.zeros(x_dim))\n",
    "prior_params['mu_prior_info'].set(mu_prior_info)\n",
    "prior_params['alpha'].set(alpha)\n",
    "prior_params['info_x'].set(info_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17115877916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.85308526,  0.77636869,  2.17115878,  1.18724932,  3.32939523])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vb_params['global']['info'].set_free(np.random.random(vb_params['global']['info'].free_size()))\n",
    "\n",
    "mu_info = vb_params['global']['info'].get()\n",
    "\n",
    "print(np.linalg.slogdet(mu_info[2])[1])\n",
    "np.linalg.slogdet(mu_info)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define entropies\n",
    "def mu_entropy(mu_info):\n",
    "    return 0.5 * np.sum(np.linalg.slogdet(mu_info)[1])\n",
    "\n",
    "def beta_entropy(tau): \n",
    "    digamma_tau0 = sp.special.digamma(tau[:, 0])\n",
    "    digamma_tau1 = sp.special.digamma(tau[:, 1])\n",
    "    digamma_tausum = sp.special.digamma(np.sum(tau, 1))\n",
    "\n",
    "    lgamma_tau0 = sp.special.gammaln(tau[:, 0])\n",
    "    lgamma_tau1 = sp.special.gammaln(tau[:, 1])\n",
    "    lgamma_tausum = sp.special.gammaln(np.sum(tau, 1))\n",
    "\n",
    "    lbeta = lgamma_tau0 + lgamma_tau1 - lgamma_tausum\n",
    "\n",
    "    return np.sum(\n",
    "        lbeta - \\\n",
    "        (tau[:, 0] - 1.) * digamma_tau0 - \\\n",
    "        (tau[:, 1] - 1.) * digamma_tau1 + \\\n",
    "        (tau[:, 0] + tau[:, 1] - 2) * digamma_tausum)\n",
    "\n",
    "def multinom_entropy(e_z): \n",
    "    return -1 * np.sum(e_z * np.log(e_z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define priors\n",
    "def dp_prior(alpha, e_log_1mv):\n",
    "    return (alpha - 1) * np.sum(e_log_1mv)\n",
    "    \n",
    "def normal_prior(mu, info, prior_mu, prior_info): \n",
    "    mu_centered = mu - prior_mu\n",
    "    return - 0.5 * np.sum(np.einsum('ki, ij, kj -> k', mu_centered, prior_info, mu_centered)  + \\\n",
    "                          np.array([np.dot(np.diag(info[k]), np.diag(prior_info)) for k in range(k_approx)]))\n",
    "                          # np.einsum('kii, ii -> k', info, prior_info)) # autograd doesn't like this last term?\n",
    "\n",
    "    #return np.sum(np.expand_dims(np.einsum('ki, ij, kj -> k', mu_centered, prior_info, mu_centered), axis=0) + \\\n",
    "    #                  np.einsum('kii, ii -> k', info, prior_info)) # double check this at some point ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 190.34603576   95.17301788    0.          -95.17301788 -190.34603576]\n",
      " [ 104.58992894   52.29496447    0.          -52.29496447 -104.58992894]\n",
      " [ 217.75763691  108.87881846    0.         -108.87881846 -217.75763691]\n",
      " [ 185.81214047   92.90607023    0.          -92.90607023 -185.81214047]\n",
      " [ 225.61715877  112.80857938    0.         -112.80857938 -225.61715877]\n",
      " [ 142.13139209   71.06569604    0.          -71.06569604 -142.13139209]\n",
      " [  62.43657429   31.21828715    0.          -31.21828715  -62.43657429]\n",
      " [ 202.33715318  101.16857659    0.         -101.16857659 -202.33715318]\n",
      " [ 231.65880635  115.82940317    0.         -115.82940317 -231.65880635]\n",
      " [ 183.35998985   91.67999493    0.          -91.67999493 -183.35998985]\n",
      " [ 104.61114086   52.30557043    0.          -52.30557043 -104.61114086]\n",
      " [  88.72918471   44.36459236    0.          -44.36459236  -88.72918471]]\n",
      "[[ 190.34603576   95.17301788    0.          -95.17301788 -190.34603576]\n",
      " [ 104.58992894   52.29496447    0.          -52.29496447 -104.58992894]\n",
      " [ 217.75763691  108.87881846    0.         -108.87881846 -217.75763691]\n",
      " [ 185.81214047   92.90607023    0.          -92.90607023 -185.81214047]\n",
      " [ 225.61715877  112.80857938    0.         -112.80857938 -225.61715877]\n",
      " [ 142.13139209   71.06569604    0.          -71.06569604 -142.13139209]\n",
      " [  62.43657429   31.21828715    0.          -31.21828715  -62.43657429]\n",
      " [ 202.33715318  101.16857659    0.         -101.16857659 -202.33715318]\n",
      " [ 231.65880635  115.82940317    0.         -115.82940317 -231.65880635]\n",
      " [ 183.35998985   91.67999493    0.          -91.67999493 -183.35998985]\n",
      " [ 104.61114086   52.30557043    0.          -52.30557043 -104.61114086]\n",
      " [  88.72918471   44.36459236    0.          -44.36459236  -88.72918471]]\n"
     ]
    }
   ],
   "source": [
    "mu = vb_params['global']['mu'].get()\n",
    "info = vb_params['global']['info'].get()\n",
    "tau = vb_params['global']['v_sticks'].alpha.get()\n",
    "\n",
    "prior_mu = prior_params['mu_prior_mean'].get()\n",
    "prior_info = prior_params['mu_prior_info'].get()\n",
    "info_x = prior_params['info_x'].get()\n",
    "\n",
    "print(np.einsum('ni, ij, kj -> nk', x, info_x, mu) )\n",
    "print(np.dot(np.dot(x, info_x), mu.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglik_sticks_by_k(e_log_v, e_log_1mv): \n",
    "    k_approx = np.shape(e_log_v)[0] + 1\n",
    "    e_log_stick_remain = np.array([np.sum(e_log_1mv[0:k]) for k in range(k_approx)])\n",
    "    e_log_new_stick = np.concatenate((e_log_v, np.array([0]))) \n",
    "    \n",
    "    return e_log_stick_remain + e_log_new_stick\n",
    "    # return np.sum(np.dot(e_z, e_log_stick_remain + e_log_new_stick))\n",
    "    \n",
    "def loglik_obs_by_nk(mu, info, x, info_x):\n",
    "    return np.einsum('ni, ij, kj -> nk', x, info_x, mu) + \\\n",
    "            - 0.5 * np.einsum('ki, ij, kj -> k', mu, info_x, mu) + \\\n",
    "            - 0.5 * np.array([np.dot(np.diag(info[k]), np.diag(info_x)) for k in range(k_approx)])\n",
    "        \n",
    "            # - 0.5 * np.einsum('kii, ii -> k', info, info_x) # autograd doesn't like this\n",
    "\n",
    "def compute_elbo(x, vb_params, prior_params): \n",
    "    e_log_v = vb_params['global']['v_sticks'].e_log()[:,0] # E[log v]\n",
    "    e_log_1mv = vb_params['global']['v_sticks'].e_log()[:,1] # E[log 1 - v]\n",
    "    e_z = vb_params['local']['e_z'].get()\n",
    "    mu = vb_params['global']['mu'].get()\n",
    "    info = vb_params['global']['info'].get()\n",
    "    tau = vb_params['global']['v_sticks'].alpha.get()\n",
    "    \n",
    "    prior_mu = prior_params['mu_prior_mean'].get()\n",
    "    prior_info = prior_params['mu_prior_info'].get()\n",
    "    info_x = prior_params['info_x'].get()\n",
    "    \n",
    "    prior = dp_prior(alpha, e_log_1mv) + normal_prior(mu, info, prior_mu, prior_info)\n",
    "    \n",
    "    # log likelihood of stick breaking process\n",
    "    log_lik_ind = np.sum(np.dot(e_z, loglik_sticks_by_k(e_log_v, e_log_1mv)))\n",
    "    \n",
    "    # log likelihood of data generating process\n",
    "    log_lik_obs = np.sum(e_z * loglik_obs_by_nk(mu, info, x, info_x))\n",
    "    \n",
    "    # log likelihood \n",
    "    log_lik = log_lik_obs + log_lik_ind + prior\n",
    "    \n",
    "    # entropy terms\n",
    "    entropy = mu_entropy(info) + beta_entropy(tau) + multinom_entropy(e_z)\n",
    "    \n",
    "    return log_lik + entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_log_v = vb_params['global']['v_sticks'].e_log()[:,0] # E[log v]\n",
    "e_log_1mv = vb_params['global']['v_sticks'].e_log()[:,1] # E[log 1 - v]\n",
    "e_z = vb_params['local']['e_z'].get()\n",
    "mu = vb_params['global']['mu'].get()\n",
    "info = vb_params['global']['info'].get()\n",
    "tau = vb_params['global']['v_sticks'].alpha.get()\n",
    "\n",
    "prior_mu = prior_params['mu_prior_mean'].get()\n",
    "prior_info = prior_params['mu_prior_info'].get()\n",
    "info_x = prior_params['info_x'].get()\n",
    "\n",
    "\n",
    "np.shape(loglik_obs_by_nk(mu, info, x, info_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_z(mu, info, x, info_x, e_log_v, e_log_1mv): \n",
    "    propto = np.exp(loglik_obs_by_nk(mu, info, x, info_x) + loglik_sticks_by_k(e_log_v, e_log_1mv))\n",
    "    const = np.sum(propto, axis = 1)\n",
    "    return propto / const[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558.12959614752617"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_elbo(x, vb_params, prior_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DP_normal_mixture(object):\n",
    "    def __init__(self, x, vb_params, prior_params):\n",
    "        self.x = x\n",
    "        self.vb_params = deepcopy(vb_params)\n",
    "        self.prior_params = deepcopy(prior_params)\n",
    "        # self.weights = np.full((x.shape[0], 1), 1.0)\n",
    "        # self.get_moment_jacobian = \\\n",
    "        #     autograd.jacobian(self.get_interesting_moments)\n",
    "\n",
    "    def optimize_z(self):\n",
    "        # Take a CAVI step on Z.\n",
    "        mu = vb_params['global']['mu'].get()\n",
    "        info = vb_params['global']['info'].get()\n",
    "        \n",
    "        info_x = prior_params['info_x'].get()\n",
    "        e_log_v = vb_params['global']['v_sticks'].e_log()[:,0] # E[log v]\n",
    "        e_log_1mv = vb_params['global']['v_sticks'].e_log()[:,1] # E[log 1 - v]\n",
    "        \n",
    "        e_z = optimize_z(mu, info, self.x, info_x, e_log_v, e_log_1mv)\n",
    "        self.vb_params['local']['e_z'].set(e_z)\n",
    "\n",
    "    def kl(self, verbose=False):\n",
    "        self.optimize_z()\n",
    "        elbo = compute_elbo(self.x, self.vb_params, self.prior_params)\n",
    "        if verbose:\n",
    "            print('ELBO:\\t', elbo)\n",
    "        return -1 * elbo\n",
    "    \n",
    "    \n",
    "    #######################\n",
    "    # TODO: Moments for sensitivity\n",
    "\n",
    "    #def get_interesting_moments(self, global_free_params):\n",
    "    #    self.params['global'].set_free(global_free_params)\n",
    "    #    self.optimize_z()\n",
    "    #    return self.params['global']['mu'].get_vector()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VariationalBayes.SparseObjectives import SparseObjective, Objective\n",
    "\n",
    "model = DP_normal_mixture(x, vb_params, prior_params)\n",
    "kl_obj = Objective(model.vb_params['global'], model.kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELBO:\t 558.138048408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-558.13804840766409"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimize_z()\n",
    "model.kl(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "-730.188645128\n"
     ]
    }
   ],
   "source": [
    "def minimize_kl(kl_obj, init_x):\n",
    "    vb_opt = optimize.minimize(\n",
    "        kl_obj.fun_free,\n",
    "        x0=init_x,\n",
    "        jac=kl_obj.fun_free_grad,\n",
    "        hess=kl_obj.fun_free_hessian,\n",
    "        method='trust-ncg', options={'maxiter': 10, 'gtol': 1e-8})\n",
    "\n",
    "    print('done')\n",
    "    return vb_opt\n",
    "\n",
    "vb_opt = minimize_kl(kl_obj, global_init_par_vec)\n",
    "print(kl_obj.fun_free(vb_opt.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.vb_params['global'].set_free(vb_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.10473365e+00,  -8.79383101e+00],\n",
       "       [ -4.25314920e+00,  -4.06284318e+00],\n",
       "       [ -5.80798436e-05,  -4.26565903e-05],\n",
       "       [ -3.93064023e-02,  -3.93064023e-02],\n",
       "       [ -7.86128046e-02,  -7.86128046e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vb_params['global']['mu'].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10., -10.],\n",
       "       [ -5.,  -5.],\n",
       "       [  0.,   0.],\n",
       "       [  5.,   5.],\n",
       "       [ 10.,  10.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
