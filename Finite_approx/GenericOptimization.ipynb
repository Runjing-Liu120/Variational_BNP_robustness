{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as sp\n",
    "from autograd.scipy import special\n",
    "from autograd import grad, hessian, hessian_vector_product\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from valez_finite_VI_lib import *\n",
    "from generic_optimization_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_approx_eq(x, y, tol=1e-12):\n",
    "    return np.max(np.abs(x - y)) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12321) # this is a seed where VI works well\n",
    "\n",
    "alpha = 10 # IBP parameter\n",
    "\n",
    "Num_samples = 5000 # sample size\n",
    "D = 2 # dimension\n",
    "# so X will be a N\\times D matrix\n",
    "\n",
    "sigma_A = 100\n",
    "\n",
    "sigma_eps = .1 # variance of noise\n",
    "\n",
    "K_inf = 3 # take to be large for a good approximation to the IBP\n",
    "\n",
    "Pi, Z, mu, A, X = generate_data(Num_samples, D, K_inf, sigma_A, sigma_eps)\n",
    "\n",
    "K_approx = deepcopy(K_inf) # variational truncation\n",
    "\n",
    "tau, nu, phi_mu, phi_var = initialize_parameters(Num_samples, D, K_approx)\n",
    "nu_init = np.round(nu * (nu >= 0.9) + nu * (nu <= 0.1)) + nu * (nu >= 0.1) * (nu <= 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, X, K_approx, alpha, sigma_eps, sigma_A):\n",
    "        self.X = X\n",
    "        self.K_approx = K_approx\n",
    "        self.alpha = alpha\n",
    "        self.data_shape = {'D': X.shape[1], 'N': X.shape[0] , 'K':K_approx}\n",
    "        self.sigmas = {'eps': sigma_eps, 'A': sigma_A}\n",
    "        #self.nu = np.empty((X.shape[0], K_approx))\n",
    "        \n",
    "    def wrapped_elbo(self, params):\n",
    "        tau, phi_mu, phi_var, nu = \\\n",
    "            unpack_params(params, self.data_shape['K'], self.data_shape['D'], self.data_shape['N'])\n",
    "        return compute_elbo(tau, nu, phi_mu, phi_var, self.X, self.sigmas, self.alpha)[0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17133393.433775857"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = pack_params(deepcopy(tau), deepcopy(phi_mu), deepcopy(phi_var), deepcopy(nu))\n",
    "data_set = DataSet(X, K_approx, alpha, sigma_eps, sigma_A)\n",
    "data_set.wrapped_elbo(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.33484173e+03   1.10801349e+04   9.87794660e+02 ...,  -2.07693646e+01\n",
      "  -2.61558804e+00   6.02003778e+01]\n"
     ]
    }
   ],
   "source": [
    "elbo_grad = grad(data_set.wrapped_elbo)\n",
    "elbo_grad = elbo_grad(params)\n",
    "print elbo_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
