{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Testing updates with autograd\n",
    "\n",
    "from valez_finite_VI_lib import *\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy as sp\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_log_likelihood(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha):\n",
    "    \n",
    "    sigma_eps = sigmas['eps']\n",
    "    sigma_A = sigmas['A']\n",
    "    D = Data_shape['D']\n",
    "    N = Data_shape['N']\n",
    "    K = Data_shape['K']\n",
    "\n",
    "    beta_lh = (alpha/K - 1.)*np.sum(E_log_pi1) \n",
    "    bern_lh = np.sum(np.dot(nu_moment[n,:], E_log_pi1) \\\n",
    "                            + np.dot(1.-nu_moment[n,:], E_log_pi2) for n in range(N))\n",
    "    Normal_A = -1/(2.*sigma_A) * np.sum(phi_moment2)\n",
    "    \n",
    "    Normal_X_sum = 0\n",
    "    ## compute the data likelihood term\n",
    "    for n in range(N): \n",
    "        dum1 = 2.*np.sum(np.sum(nu_moment[n,i] * nu_moment[n,j] * np.dot(phi_moment1[:,i],phi_moment1[:,j]) \\\n",
    "                                for i in range(j)) for j in range(K))\n",
    "        dum2 = np.dot(nu_moment[n,:] , phi_moment2 )\n",
    "        \n",
    "        dum3 = -2. * np.dot(X[n,:], np.dot(phi_moment1, nu_moment[n,:]))\n",
    "        \n",
    "        # dum4 = np.dot(X[n,:], X[n,:])\n",
    "        Normal_X_sum += dum1 + dum2 + dum3\n",
    "        \n",
    "    Normal_X = -1/(2*sigma_eps)*Normal_X_sum\n",
    "    \n",
    "    y = beta_lh + bern_lh + Normal_A + Normal_X\n",
    "    return(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Num_samples = 10 # sample size\n",
    "D = 2 # dimension\n",
    "# so X will be a n\\times D matrix\n",
    "\n",
    "K_inf = 3 # take to be large for a good approximation to the IBP\n",
    "K_approx = deepcopy(K_inf)\n",
    "\n",
    "alpha = 2 # IBP parameter\n",
    "Pi = np.zeros(K_inf)\n",
    "Z = np.zeros([Num_samples,K_inf])\n",
    "\n",
    "# Parameters to draw A from MVN\n",
    "mu = np.zeros(D)\n",
    "sigma_A = 100\n",
    "\n",
    "sigma_eps = 1 # variance of noise\n",
    "\n",
    "# Draw Z from truncated stick breaking process\n",
    "for k in range(K_inf):\n",
    "    Pi[k] = np.random.beta(alpha/K_inf,1)\n",
    "    for n in range(Num_samples):\n",
    "        Z[n,k] = np.random.binomial(1,Pi[k])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw A from multivariate normal\n",
    "# A = np.random.multivariate_normal(mu, sigma_A*np.identity(D), K_approx)\n",
    "A = np.random.normal(0, np.sqrt(sigma_A), (K_approx,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# draw noise\n",
    "# epsilon = np.random.multivariate_normal(np.zeros(D), sigma_eps*np.identity(D), Num_samples)\n",
    "epsilon = np.random.normal(0, np.sqrt(sigma_eps), (Num_samples, D))\n",
    "\n",
    "# the observed data\n",
    "X = np.dot(Z,A) + epsilon\n",
    "\n",
    "\n",
    "Data_shape = {'D':D, 'N': Num_samples , 'K':K_approx}\n",
    "sigmas = {'eps': sigma_eps, 'A': sigma_A}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Nu Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.7947076037e-19\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.04083408559e-17\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.11022302463e-16\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# initialization for cavi updates\n",
    "tau = np.random.uniform(10,100,[K_approx,2])\n",
    "nu = np.random.uniform(0,1,[Num_samples,K_approx])\n",
    "\n",
    "phi_mu = np.random.normal(0,1,[D,K_approx])\n",
    "phi_var = np.ones(K_approx)\n",
    "\n",
    "# autodiff \n",
    "d_exp_log_LH = grad(exp_log_likelihood, 0)\n",
    "\n",
    "# compute required moments\n",
    "phi_moment1 = deepcopy(phi_mu)\n",
    "phi_moment2 = np.diag(np.dot(phi_mu.T, phi_mu) + D * phi_var)\n",
    "E_log_pi1 = sp.special.digamma(tau[:,0]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "E_log_pi2 = sp.special.digamma(tau[:,1]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "\n",
    "\n",
    "for n in range(Num_samples): \n",
    "    for k in range(K_approx): \n",
    "\n",
    "        nu_moment = deepcopy(nu)\n",
    "        script_V_AG = d_exp_log_LH(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha)\n",
    "        nu_AG = 1/(1 + np.exp(-script_V_AG))\n",
    "\n",
    "        nu_updates(tau, nu, phi_mu, phi_var, X, sigmas, n, k)\n",
    "        \n",
    "        print(np.abs(nu[n,k] - nu_AG[n,k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tau updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from cavi update: \n",
      " [[ 5.30976206  4.95699562  7.64671928]\n",
      " [ 6.35690461  6.70967105  4.01994739]]\n",
      "results from autograd: \n",
      "[ 5.30976206  4.95699562  7.64671928]\n",
      "[ 6.35690461  6.70967105  4.01994739]\n"
     ]
    }
   ],
   "source": [
    "# initialization for cavi updates\n",
    "tau = np.random.uniform(10,100,[K_approx,2])\n",
    "nu = np.random.uniform(0,1,[Num_samples,K_approx])\n",
    "\n",
    "phi_mu = np.random.normal(0,1,[D,K_approx])\n",
    "phi_var = np.ones(K_approx)\n",
    "\n",
    "# calling autodiff\n",
    "d_tau1 = grad(exp_log_likelihood, 3)\n",
    "d_tau2 = grad(exp_log_likelihood, 4)\n",
    "\n",
    "# computing moments\n",
    "nu_moment = deepcopy(nu)\n",
    "phi_moment1 = deepcopy(phi_mu)\n",
    "phi_moment2 = np.diag(np.dot(phi_mu.T, phi_mu) + D * phi_var)\n",
    "E_log_pi1 = sp.special.digamma(tau[:,0]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "E_log_pi2 = sp.special.digamma(tau[:,1]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "\n",
    "# computing updates\n",
    "tau1_AG = d_tau1(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha) + 1\n",
    "tau2_AG = d_tau2(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha) + 1\n",
    "\n",
    "tau_updates(tau, nu, alpha)\n",
    "\n",
    "print('results from cavi update: \\n', tau.T)\n",
    "print('results from autograd: ')\n",
    "print(tau1_AG)\n",
    "print(tau2_AG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing phi updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean computed by autodiff: \n",
      " [-13.79939011  -5.59656266]\n",
      "mean computed by cavi: \n",
      " [-13.79939011  -5.59656266]\n",
      "variance computed by autodiff:  0.210432283724\n",
      "variance computed by cavi    :  0.210432283724\n",
      "\n",
      "\n",
      "mean computed by autodiff: \n",
      " [-5.3615298  -3.77695658]\n",
      "mean computed by cavi: \n",
      " [-5.3615298  -3.77695658]\n",
      "variance computed by autodiff:  0.254258216792\n",
      "variance computed by cavi    :  0.254258216792\n",
      "\n",
      "\n",
      "mean computed by autodiff: \n",
      " [-2.324603   -1.34020787]\n",
      "mean computed by cavi: \n",
      " [-2.324603   -1.34020787]\n",
      "variance computed by autodiff:  0.212070773662\n",
      "variance computed by cavi    :  0.212070773662\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialization for cavi updates\n",
    "tau = np.random.uniform(10,100,[K_approx,2])\n",
    "nu = np.random.uniform(0,1,[Num_samples,K_approx])\n",
    "\n",
    "phi_mu = np.random.normal(0,1,[D,K_approx])\n",
    "phi_var = np.ones(K_approx)\n",
    "\n",
    "# calling autodiff\n",
    "d_phi1  = grad(exp_log_likelihood, 1)\n",
    "d_phi2 = grad(exp_log_likelihood, 2)\n",
    "\n",
    "# compute moments\n",
    "\n",
    "E_log_pi1 = sp.special.digamma(tau[:,0]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "E_log_pi2 = sp.special.digamma(tau[:,1]) - sp.special.digamma(tau[:,0] + tau[:,1]) \n",
    "\n",
    "\n",
    "\n",
    "for k in range(K_approx): \n",
    "    nu_moment = deepcopy(nu)\n",
    "    phi_moment1 = deepcopy(phi_mu)\n",
    "    phi_moment2 = np.diag(np.dot(phi_mu.T, phi_mu) + D * phi_var)\n",
    "\n",
    "    # compute autograd updates\n",
    "    phi1_AG = d_phi1(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha) \n",
    "    phi2_AG = d_phi2(nu_moment, phi_moment1, phi_moment2, \\\n",
    "                       E_log_pi1, E_log_pi2, Data_shape, sigmas, X, alpha) \n",
    "\n",
    "    # convert to standard parametrization\n",
    "    phi_var_AG = -1/(2.*phi2_AG)\n",
    "    phi_mu_AG = np.dot(phi1_AG, np.diag(phi_var_AG))\n",
    "\n",
    "    phi_updates(nu, phi_mu, phi_var, X, sigmas, k) # cavi updates\n",
    "    \n",
    "\n",
    "    print('mean computed by autodiff: \\n', phi_mu_AG[:,k])\n",
    "    print('mean computed by cavi: \\n', phi_mu[:,k])\n",
    "    print('variance computed by autodiff: ', phi_var_AG[k])\n",
    "    print('variance computed by cavi    : ', phi_var[k])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
