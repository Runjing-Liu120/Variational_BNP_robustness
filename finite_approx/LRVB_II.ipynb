{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity analysis in the finite approximation to the IBP\n",
    "These results should be the same as in the LRVB notebook \n",
    "\n",
    "The difference being that here, functions are written using the VB library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as sp\n",
    "from autograd.scipy import special\n",
    "from autograd import grad, hessian, hessian_vector_product, hessian, jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "import time\n",
    "\n",
    "import valez_finite_VI_lib as vi\n",
    "import LRVB_lib as lrvb\n",
    "import generic_optimization_lib as packing\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../LinearResponseVariationalBayes.py')\n",
    "from VariationalBayes.ParameterDictionary import ModelParamsDict\n",
    "from VariationalBayes.Parameters import ScalarParam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12321)\n",
    "\n",
    "alpha = 10 # IBP parameter\n",
    "\n",
    "num_samples = 50 # sample size\n",
    "D = 2 # dimension\n",
    "\n",
    "sigma_a = 3.0 ** 2\n",
    "\n",
    "sigma_eps = 1.0 ** 2 # variance of noise\n",
    "\n",
    "k_inf = 3 # take to be large for a good approximation to the IBP\n",
    "\n",
    "# generate data\n",
    "pi, Z, A, X = vi.generate_data(num_samples, D, k_inf, sigma_a, sigma_eps, alpha)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variational parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_approx = k_inf # variational truncation\n",
    "\n",
    "tau_init, nu_init, phi_mu_init, phi_var_init = \\\n",
    "    vi.initialize_parameters(num_samples, D, k_approx)\n",
    "\n",
    "# define vb model -- vb_model is a class defined using the VB library\n",
    "vb_model = vi.set_ibp_vb_model(num_samples, D, k_approx)\n",
    "\n",
    "# initialize parameters\n",
    "vb_model['phi'].set_vector(np.hstack([np.ravel(phi_mu_init.T), phi_var_init]))\n",
    "vb_model['pi'].set_vector(np.ravel(tau_init))\n",
    "vb_model['nu'].set_vector(np.ravel(nu_init))\n",
    "\n",
    "# consolidate hyper parameters\n",
    "hyper_params = ModelParamsDict('hyper_params')\n",
    "hyper_params.push_param(ScalarParam('alpha', lb = 0.0))\n",
    "hyper_params.push_param(ScalarParam('var_a', lb = 0.0))\n",
    "hyper_params.push_param(ScalarParam('var_eps', lb = 0.0))\n",
    "\n",
    "hyper_params['alpha'].set(alpha)\n",
    "hyper_params['var_a'].set(sigma_a)\n",
    "hyper_params['var_eps'].set(sigma_eps)\n",
    "\n",
    "data_set = lrvb.DataSetII(X, vb_model, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_lh -10.7696647788\n",
      "e_log_lik -175.107835517\n",
      "entropy:  85.8272964727\n",
      "-89.2805390439\n",
      "beta_lh -9.28728092833\n",
      "e_log_lik -173.625451666\n",
      "entropy 85.8272964727\n",
      "-87.7981551935\n"
     ]
    }
   ],
   "source": [
    "import functional_perturbation_lib as fun_pert\n",
    "\n",
    "print(fun_pert.compute_elbo_perturbed(X, vb_model, hyper_params))\n",
    "\n",
    "print(vi.compute_elboII(X, vb_model, hyper_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.59343037 -0.50521162 -0.82858936]\n",
      "(-0.5934303703738715, 1.0571672779123879e-13)\n",
      "(-0.5052116199254114, 1.4845643601425502e-12)\n",
      "(-0.8285893603156798, 1.5636347128151992e-12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.41272211,  3.78621041,  3.20580405])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import integrate\n",
    "from scipy.stats import beta\n",
    "import scipy as osp\n",
    "\n",
    "tau = np.random.uniform(20, 40, [k_approx, 2])\n",
    "\n",
    "print(fun_pert.compute_e_pi_prior_perturbed(tau, alpha, k_approx, n_grid = 10**6))\n",
    "\n",
    "for k in range(k_approx): \n",
    "    integrand = lambda x : beta.pdf(x, tau[k,0], tau[k,1]) * beta.logpdf(x, alpha/k_approx, 1) \n",
    "    print(integrate.quad(integrand, 0,1))\n",
    "\n",
    "(alpha/k_approx - 1) * osp.special.digamma(tau[:,0]) - osp.special.digamma(tau[:,0] + tau[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.413370616981\n",
      "-1.61734342131\n"
     ]
    }
   ],
   "source": [
    "x = 0.5\n",
    "\n",
    "print(beta.logpdf(x, alpha/k_approx, 1) )\n",
    "\n",
    "print((alpha/k_approx - 1) * np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just checking some things about vb_model\n",
    "\n",
    "print('vb_model[phi] is an array of normals; \\neach component has a mean, and each row has a common variance')\n",
    "print(vb_model['phi'].mean.get())\n",
    "print(vb_model['phi'].info.get(), '\\n')\n",
    "assert np.all(phi_mu_init.T == vb_model['phi'].e())\n",
    "\n",
    "print('vb_model[pi] is an array where each row is dirichlet \\nwe can get its \\\n",
    "dirchlet parameter tau, and we can get its mean')\n",
    "print(vb_model['pi'].alpha.get())\n",
    "print(vb_model['pi'].e(), '\\n')\n",
    "assert np.all(tau_init == vb_model['pi'].alpha.get())\n",
    "\n",
    "print('vb_model[pi] is an ArrayParam class where each element is constrained to be between 0 and 1')\n",
    "print(vb_model['nu'].get())\n",
    "assert np.all(nu_init == vb_model['nu'].get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = 0\n",
    "plt.figure()\n",
    "plt.hist(data_set.x[:, col], bins=100);\n",
    "\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "plt.figure()\n",
    "plt.plot(data_set.x[:, col1], data_set.x[:, col2], 'k.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CAVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters approximating the true distribution\n",
    "\n",
    "tau_true = np.zeros_like(tau_init)\n",
    "tau_true_scale = 15.\n",
    "tau_true[:, 0] = deepcopy(pi) * tau_true_scale\n",
    "tau_true[:, 1] = tau_true_scale\n",
    "\n",
    "nu_true = np.zeros_like(nu_init)\n",
    "nu_true[Z == 1] = 0.999\n",
    "nu_true[Z == 0] = 0.001\n",
    "\n",
    "phi_mu_true = np.zeros_like(phi_mu_init)\n",
    "phi_mu_true[:] = A.transpose()\n",
    "phi_var_true = np.zeros_like(phi_var_init)\n",
    "phi_var_true[:] = 0.01\n",
    "\n",
    "params_true = packing.pack_params(deepcopy(tau_true), deepcopy(phi_mu_true),\n",
    "                                  deepcopy(phi_var_true), deepcopy(nu_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_init = False\n",
    "if true_init:\n",
    "    vb_model['phi'].set_vector(np.hstack([np.ravel(phi_mu_true.T), phi_var_true]))\n",
    "    vb_model['pi'].set_vector(np.ravel(tau_true))\n",
    "    vb_model['nu'].set_vector(np.ravel(nu_true))\n",
    "    tau, phi_mu, phi_var, nu = data_set.unpack_params(vb_model)\n",
    "else:\n",
    "    # the random initialization was done above\n",
    "    tau, phi_mu, phi_var, nu = data_set.unpack_params(vb_model)\n",
    "    \n",
    "\n",
    "    tau, nu, phi_mu, phi_var = data_set.run_cavi(tau, nu, phi_mu, phi_var, max_iter=100, tol=1e-6)\n",
    "\n",
    "cavi_tau = deepcopy(tau)\n",
    "cavi_phi_mu = deepcopy(phi_mu)\n",
    "cavi_phi_var = deepcopy(phi_var)\n",
    "cavi_nu = deepcopy(nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CAVI can return nu values that are too close to 0 or 1 for the encoding.\n",
    "nu_tol = 1e-8\n",
    "cavi_nu_trim = deepcopy(cavi_nu)\n",
    "cavi_nu_trim[cavi_nu_trim < nu_tol] = nu_tol\n",
    "cavi_nu_trim[cavi_nu_trim > 1 - nu_tol] = 1 - nu_tol\n",
    "\n",
    "cavi_params = packing.pack_params(cavi_tau, cavi_phi_mu, cavi_phi_var, cavi_nu_trim)\n",
    "print(np.all(np.isfinite(cavi_params)))\n",
    "\n",
    "vb_model.set_free(cavi_params)\n",
    "cavi_resid = data_set.x - data_set.get_prediction(vb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set.trace.reset()\n",
    "vb_opt = data_set.run_newton_tr(cavi_params, maxiter=50, gtol=1e-2)\n",
    "vb_model.set_free(vb_opt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau_final = vb_model['pi'].alpha.get()\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "x = np.linspace(.001, 0.999, 100)\n",
    "y = beta.pdf(x, tau_final[0,0], tau_final[0,1])\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('CAVI:')\n",
    "print (cavi_phi_mu.transpose())\n",
    "\n",
    "print ('Full TR:')\n",
    "print (vb_model['phi'].mean.get())\n",
    "\n",
    "print ('Truth:')\n",
    "print (A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_resid = data_set.x - data_set.get_prediction(vb_model)\n",
    "true_resid = data_set.x - np.matmul(Z, A)\n",
    "\n",
    "plt.figure()\n",
    "col = 0\n",
    "plt.plot(data_set.x[:, col], tr_resid[:, col], '.b')\n",
    "plt.plot(data_set.x[:, col], cavi_resid[:, col], '.r')\n",
    "plt.plot(data_set.x[:, col], true_resid[:, col], '.g')\n",
    "plt.plot(data_set.x[:, col], np.full_like(data_set.x[:, col], 0.), 'k')\n",
    "\n",
    "print('Cavi residuals: {}    Trust residuals: {}      True residuals: {}'.format(\n",
    "       np.sum(np.abs(cavi_resid)), np.sum(np.abs(tr_resid)), np.sum(np.abs(true_resid))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute prior sensitivity, eq. 18 in the paper\n",
    "moment_sensitivity = data_set.local_prior_sensitivity()\n",
    "\n",
    "#print(moment_sensitivity)\n",
    "#np.shape(moment_sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The third column is sigma_eps.\n",
    "sigma_eps_col = 2\n",
    "e_log_pi_sigma_eps_sens, e_mu_sigma_eps_sens = \\\n",
    "    packing.unpack_moments(moment_sensitivity[:, sigma_eps_col], k_approx, D)\n",
    "    \n",
    "print('Sensitivity of e_log_pi to sigma_eps:')\n",
    "print(e_log_pi_sigma_eps_sens)\n",
    "\n",
    "print('Sensitivity of e_mu to sigma_eps:')\n",
    "print(e_mu_sigma_eps_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# Perturb and re-rerun to check the sensitivity.\n",
    "\n",
    "epsilon = 1e-1\n",
    "data_set_perturb = lrvb.DataSet(X, k_approx, alpha, sigma_eps + epsilon, sigma_a)\n",
    "data_set_perturb.trace.print_every = 1\n",
    "vb_opt_perturb = data_set_perturb.run_newton_tr(vb_opt.x)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"e_log_pi, e_mu = data_set.get_moments(tr_params)\n",
    "e_log_pi_perturb, e_mu_perturb = data_set.get_moments(vb_opt_perturb.x)\n",
    "\n",
    "print('Measured sensitivity of e_mu to sigma_eps:')\n",
    "print((e_mu_perturb - e_mu) / epsilon)\n",
    "\n",
    "print('Sensitivity of e_mu to sigma_eps:')\n",
    "print(e_mu_sigma_eps_sens)\n",
    "\n",
    "print('Measured sensitivity of e_log_pi to sigma_eps:')\n",
    "print((e_log_pi_perturb - e_log_pi) / epsilon)\n",
    "\n",
    "print('Sensitivity of e_log_pi to sigma_eps:')\n",
    "print(e_log_pi_sigma_eps_sens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set.influence_function_pi(0.8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot influence function\n",
    "\n",
    "n_ticks = 100\n",
    "y = np.zeros((n_ticks, k_approx))\n",
    "\n",
    "post_pi = 0 # which pi you're looking at in the posterior\n",
    "\n",
    "theta = np.linspace(0.001,0.999,n_ticks)\n",
    "\n",
    "for k in range(k_approx):  \n",
    "    for i in range(n_ticks): \n",
    "        y[i, k] = data_set.influence_function_pi(theta[i], k)[post_pi]\n",
    "        \n",
    "    plt.plot(theta, y[:,k])\n",
    "    plt.xlabel('theta')\n",
    "    plt.title('Marginal Influence on E[log pi' + str(post_pi) + '],\\nwith prior perturbation on pi_' + str(k))\n",
    "    plt.show()\n",
    "\n",
    "plt.plot(theta, np.sum(y,1))\n",
    "plt.xlabel('theta')\n",
    "plt.title('Total Influence on E[log pi' + str(post_pi) + ']')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vi.compute_elboII(X, vb_model, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
